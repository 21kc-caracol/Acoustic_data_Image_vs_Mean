{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image vs Math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing to dark theme- used once then commented.  # https://github.com/dunovank/jupyter-themexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ! pip install jupyterthemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! jt -t monokai\n",
    "# ! jt -t onedork   # nicer colors for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new imports \n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import IPython.display as ipd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# imports from prev proj\n",
    "import sys\n",
    "import librosa\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import warnings  # record warnings from librosa\n",
    "from tqdm import tqdm\n",
    "from audioread import NoBackendError\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from pydub.utils import make_chunks\n",
    "from pydub import AudioSegment\n",
    "\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, Dropout, MaxPooling2D, Activation,Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import load_model\n",
    "\n",
    "    \n",
    "    \n",
    "from shutil import copyfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  global objects\n",
    "# todo add private/public inside class\n",
    "class global_For_Clf():\n",
    "    def __init__(self, clf_label):\n",
    "        #  changed for every class (for example: scream, cry, ...)\n",
    "        self.clf_label = clf_label  # have to create a clf with a label\n",
    "\n",
    "        # keeping the hardcoded 20 mfcc below until end of project submission, later update it to generic mfcc amount\n",
    "        self.data_file_path = 'csv/'+str(self.get_clf_label())+'/data_'+str(self.get_clf_label())+'_mfcc_20.csv'\n",
    "        self.csv_to_pkl_path = 'pickle/'+str(self.get_clf_label())+'/combined_lower_amount.pkl' # relevant to modular file TODO currently this is only for scream\n",
    "        self.path_csv_train_test_data = 'csv/'+str(self.get_clf_label())+'/train_test_data.csv'  # chosen 1:1 ratio data, selected from data.csv\n",
    "        self.resultsPath = 'results/'+str(self.get_clf_label())+'/experiments_results.csv'\n",
    "\n",
    "        # end of class changes\n",
    "\n",
    "        self.n_mfcc = 20  # lev's initial value here was 40- this is the feature resolution- usually between 12-40\n",
    "        self.k_folds = 5  # amount of folds in k-fold\n",
    "        # inside create_csv() more columns will be added to the csv head\n",
    "        # TODO lev-future_improvement edit/add to get better results\n",
    "        self.csv_initial_head = 'filename spectral_centroid zero_crossings spectral_rolloff chroma_stft rms mel_spec'\n",
    "\n",
    "        self.min_wav_duration = 0.5  # wont use shorter wav files\n",
    "\n",
    "        self.nearMissRatio = 2  # 2 means <positives amount>/2\n",
    "        #                           which means were taking 50% from nearMiss_<clf label> for negatives\n",
    "\n",
    "        self.nearMiss_samples = -1  # -1 is initial invalid value which will be changed on relevant functions\n",
    "        self.nearMissLabel = \"NearMiss_\" + str(self.clf_label)\n",
    "\n",
    "        self.Kfold_testSize = 0.2\n",
    "\n",
    "        self.sampling_data_repetitions = 5  # sampling randomly the data to create 1:1 ratio\n",
    "        self.k_fold_repetitions: int = 5  # doing repeated k-fold for better evaluation\n",
    "\n",
    "        self.positives = -1  # -1 represents invalid value as initial value\n",
    "        self.negatives = -1\n",
    "\n",
    "        self.try_lower_amount = np.inf\n",
    "\n",
    "        self.model = None  # here a model will be saved- the saved model shouldn't be trained\n",
    "        self.finalModelsPath = 'models/final_models'\n",
    "        self.isTrained = False\n",
    "\n",
    "        self.userInput = ''\n",
    "        \n",
    "        self.split_by_sec = 3  # split every X seconds\n",
    "        self.tested_file_name = ''  # name of file for prediction\n",
    "        self.predictor_pos_percent_condition = 0  # above 0.XX will consider as positive prediction 0 means dont use this\n",
    "\n",
    "\n",
    "    def getInputDim(self):\n",
    "        amount = len(self.csv_initial_head.split()) + self.n_mfcc - 1  # -1 because filename isnt a feature\n",
    "        return amount\n",
    "\n",
    "    def get_total_samples(self):\n",
    "        return self.positives + self.negatives\n",
    "\n",
    "    def get_model_name(self):\n",
    "        model_name = (type(self.model)).__name__\n",
    "        return model_name\n",
    "\n",
    "    def get_clf_label(self):\n",
    "            return self.clf_label\n",
    "\n",
    "\n",
    "#  exceptions\n",
    "class NotEnoughPositiveSamples(Exception):\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "\n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature_to_csv(wav_path, label, data_file_path, min_wav_duration, fcc_amount):\n",
    "    \"\"\"\n",
    "\n",
    "    :return: writes one row to wav_path with extracted features\n",
    "\n",
    "    \"\"\"\n",
    "    # extract features for a wav file\n",
    "    wav_name = wav_path.name  # 110142__ryding__scary-scream-4.wav\n",
    "    wav_name = wav_name.replace(\" \", \"_\")  # lev bug fix to align csv columns\n",
    "\n",
    "    \"\"\"\n",
    "    # lev upgrading error tracking- know which file caused the error\n",
    "    try:\n",
    "    \"\"\"\n",
    "    wav_data, sampling_rate = librosa.load(wav_path, duration=5, sr=22050)\n",
    "\n",
    "    wav_duration = librosa.get_duration(y=wav_data, sr=sampling_rate)\n",
    "\n",
    "    # lev- dont use really short audio\n",
    "    if (wav_duration < min_wav_duration):\n",
    "        print(\"skipping \" + wav_name + \" ,duration= \" + str(wav_duration))\n",
    "        return\n",
    "\n",
    "    with warnings.catch_warnings(record=True) as feature_warnings:\n",
    "        #  spectral_centroid\n",
    "        feature_wav_spec_cent = librosa.feature.spectral_centroid(y=wav_data, sr=sampling_rate)\n",
    "        #  print(feature_wav_spec_cent.shape)  #  (1, 216)\n",
    "\n",
    "        #  zero crossings\n",
    "        zcr = librosa.feature.zero_crossing_rate(wav_data)\n",
    "        #  print(\"sum \"+ str(np.sum(zcr)))\n",
    "\n",
    "        #  spectral_rolloff\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=wav_data, sr=sampling_rate)\n",
    "        # print(rolloff.shape)\n",
    "        # print(rolloff[0][0:3])\n",
    "\n",
    "        #  chroma_stft\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=wav_data, sr=sampling_rate)\n",
    "        #  print(chroma_stft.shape)\n",
    "\n",
    "        #  rms and mfccs\n",
    "        n_mfcc = fcc_amount  # resolution amount\n",
    "        mfccs = librosa.feature.mfcc(y=wav_data, sr=sampling_rate, n_mfcc=n_mfcc)\n",
    "        S, phase = librosa.magphase(mfccs)\n",
    "        rms = librosa.feature.rms(S=S)\n",
    "        #  print(rms.shape)\n",
    "\n",
    "        # mel spectogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=wav_data, sr=sampling_rate)\n",
    "\n",
    "        # mfccs\n",
    "        #  print(mfccs.shape)\n",
    "        # if there ara warnings- print and continue- for example Warning: Trying to estimate tuning from empty frequency set\n",
    "        # this is an OK warning- it just means that its really quiet..as in street ambient during the evenning..its a\n",
    "        # good negative example.\n",
    "        if len(feature_warnings) > 0:\n",
    "            for feature_warning in feature_warnings:\n",
    "                print(\"Warning: {} Triggered in:\\n {}\\nwith a duration of {} seconds.\\n\".format(\n",
    "                    feature_warning.message, wav_path, wav_duration))\n",
    "\n",
    "        # got here - no warnings for this wav_path\n",
    "        # normalize what isnt normalized\n",
    "        to_append = f'{wav_name} {np.mean(feature_wav_spec_cent)} {np.mean(zcr)} {np.mean(rolloff)} {np.mean(chroma_stft)}' \\\n",
    "                    f' {np.mean(rms)} {np.mean(mel_spec)}'\n",
    "        for e in mfccs:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "\n",
    "        to_append += f' {label}'\n",
    "\n",
    "        #  save to csv (append new lines)\n",
    "        file = open(data_file_path, 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())\n",
    "\n",
    "        #  print(to_append)\n",
    "\n",
    "\n",
    "def create_csv(data_file_path, min_wav_duration, n_mfcc_number, header, fcc_amount,source=\"train\"):\n",
    "    \"\"\"\n",
    "    input: uses screamGlobals for input\n",
    "    output: .csv file with screamGlobals.csv_initial_head columns\n",
    "    \"\"\"\n",
    "    # important variables\n",
    "    # data_file_path = clfGlobals.data_file_path\n",
    "    # min_wav_duration = clfGlobals.min_wav_duration\n",
    "    #  print(data_file_path, min_wav_duration)\n",
    "    \"\"\"\n",
    "    #  prevent data file over run by accident\n",
    "    if os.path.exists(data_file_path):\n",
    "        text = input(f'Press the space bar to override {data_file_path} and continue with the script')\n",
    "        if text != ' ':\n",
    "            sys.exit('User aborted script, data file saved :)')\n",
    "    \"\"\"\n",
    "    # logic modification- just verify corectness if file exists- never override.\n",
    "    if os.path.exists(data_file_path):\n",
    "        # verify table fits the mfcc number- if True- return (continue with script as usuall), else- raise Error\n",
    "        # n_mfcc_number = clfGlobals.n_mfcc\n",
    "        with open(data_file_path) as csvFile:\n",
    "            reader = csv.reader(csvFile)\n",
    "            field_names_list = next(reader)  # read first row only (header)\n",
    "            mfcc_list = [x for x in field_names_list if x.startswith(\"mfcc\")]\n",
    "            len_actual_mfcc_features = len(mfcc_list)\n",
    "        if len_actual_mfcc_features == n_mfcc_number:\n",
    "            print(f'OK: {len_actual_mfcc_features} ==  n_mfcc_number={n_mfcc_number}')\n",
    "            return\n",
    "        else:\n",
    "            raise Exception(f'len_actual_mfcc_features'\n",
    "                            f'(mfcc inside {data_file_path}={len_actual_mfcc_features},'\n",
    "                            f' but n_mfcc_number(inside globals class of this script)={n_mfcc_number},'\n",
    "                            f' values must be equal.')\n",
    "\n",
    "    # create header for csv\n",
    "    # header = clfGlobals.csv_initial_head\n",
    "    # fcc_amount = clfGlobals.n_mfcc\n",
    "    for i in range(1, fcc_amount + 1):\n",
    "        header += f' mfcc_{i}'\n",
    "    header += ' label'\n",
    "    header = header.split()  # split by spaces as default\n",
    "\n",
    "    file = open(data_file_path, 'w', newline='')\n",
    "    with file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "\n",
    "    # load features from each wav file- put inside the lines below as a function\n",
    "\n",
    "    # reaching each wav file\n",
    "    path_train = Path(source)\n",
    "    for path_label in sorted(path_train.iterdir()):\n",
    "        print(\"currently in : \" + str(path_label))  # train\\negative\n",
    "        positiveOrNegative = path_label.name  # negative\n",
    "        #  print(label)\n",
    "        for path_class in tqdm(sorted(path_label.iterdir())):\n",
    "            # print info\n",
    "            print(\"currently in class: \" + str(path_class))\n",
    "            # print amount of files in directory\n",
    "            onlyfiles = next(os.walk(path_class))[2]  # dir is your directory path as string\n",
    "            wav_amount: int = len(onlyfiles)\n",
    "            print(\"wav amount= \" + str(wav_amount))\n",
    "            #  true_class= path_class.name\n",
    "            #  print(true_class)\n",
    "            #  print(path_class)  #  train\\negative\\scream\n",
    "            #  print(\"name: \"+ str(path_class.name))\n",
    "\n",
    "            # lev improvement according to coordination with mori- irrelevant since 7.8.19\n",
    "            if (positiveOrNegative == \"positive\"):\n",
    "                label = path_class.name  # scream\n",
    "            else:\n",
    "                \"\"\"\n",
    "                lev- updating to differentiate near misses and far misses.\n",
    "                keeping if-else structure for future options\n",
    "\n",
    "                old:\n",
    "                print(f\"switching label from {path_class.name} to <negative>\")  # added reporting\n",
    "                label = \"negative\"\n",
    "                new:\n",
    "\n",
    "                \"\"\"\n",
    "                label = path_class.name  # NearMiss_scream\n",
    "\n",
    "            wave_file_paths = path_class.glob('**/*.wav')  # <class 'generator'>\n",
    "            #  print(type(wave_file_paths))\n",
    "            count = 0  # for progress tracking\n",
    "            print('covered WAV files: ')\n",
    "            for wav_path in sorted(wave_file_paths):\n",
    "                wav_path = Path(wav_path)\n",
    "                count += 1\n",
    "                if (count % 50) == 0:\n",
    "                    fp = sys.stdout\n",
    "                    print(str(count), end=' ')\n",
    "                    fp.flush()  # makes print flush its buffer (doesnt print without it)\n",
    "                #  print(type(wav_path))  #  <class 'pathlib.WindowsPath'>\n",
    "                #  print(wav_path)  #  train\\positive\\scream\\110142__ryding__scary-scream-4.wav\n",
    "                #  print(wav_path.name)  #  110142__ryding__scary-scream-4.wav\n",
    "                try:\n",
    "                    #  keeping as parameters data_file_path, min_wav_duration even though its in screamGlobals\n",
    "                    #  in order to emphasis its an inner function of create_csv()\n",
    "                    extract_feature_to_csv(wav_path, label, data_file_path, min_wav_duration, fcc_amount)\n",
    "                except NoBackendError as e:\n",
    "                    print(\"audioread.NoBackendError \" + \"for wav path \" + str(wav_path))\n",
    "                    continue  # one file didnt work, continue to next one\n",
    "\n",
    "                    \n",
    "def create_lower_bound_data_panda(csv_path, label, clf_lowerAmount, clf_train_test_data):\n",
    "    \"\"\"\n",
    "    note(lev): because usually we will have more negatives than positives then this function\n",
    "        chooses randomly the negatives samples so that it will have 1:1 ratio with the true label\n",
    "        and within the amount of false labels, it Stratifies to keep the same ratio of\n",
    "        Near Misses for both train and test data.\n",
    "        (this has proven to increase the k-fold average accuracy from 0.45 to 0.85\n",
    "\n",
    "    if supplied a lower_bound which is lower than the negatives or positives amount it will\n",
    "    act as above but with |\"lowe bound\"| positives and |\"lower bound\"| negatives\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'choosing max samples randomly while preserving 1:1 ratio for {label}:<all the rest as one group>')\n",
    "    # use Pandas package for reading csv\n",
    "    data_csv = pd.read_csv(csv_path)\n",
    "    # print(data_csv[data_csv.label == 'scream'])  #  [367 rows x 47 columns]\n",
    "    # print(len(data_csv[data_csv.label == 'scream']))  # 367\n",
    "\n",
    "    # find lower amount from types of labels\n",
    "\n",
    "    pos_amount = len(data_csv[data_csv.label == label])\n",
    "    neg_amount = len(data_csv[data_csv.label != label])\n",
    "    print(\"positives: \" + str(pos_amount) + \" negatives: \" + str(neg_amount))\n",
    "    lower_amount = min(pos_amount, neg_amount, clf_lowerAmount)\n",
    "    print(\"lower bound: \" + str(lower_amount))\n",
    "\n",
    "    \"\"\"\n",
    "    in my previous project I used near misses, but in the project \"Image vs Math i'm starting with \n",
    "    simplified experiments- so i'm changing the logic to use only Far_miss\"\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        # take Max of 50% from NearMiss_<clf label> and then choose randomly from the rest of negatives\n",
    "    # lev bug fix should take lower amount as the Numerator\n",
    "    #nearMissMaxAmount = pos_amount // screamGlobals.nearMissRatio\n",
    "    nearMissMaxAmount = lower_amount // clfGlobals.nearMissRatio\n",
    "    #  print(\"near miss max amount: \",nearMissMaxAmount)\n",
    "\n",
    "    data_csv_negatives_nearMiss = data_csv.loc[data_csv.label == clfGlobals.nearMissLabel, :]  # take all valid rows\n",
    "    nearMissActualAmount = len(data_csv_negatives_nearMiss)\n",
    "    NearMissAmountToTake = nearMissActualAmount if nearMissActualAmount < nearMissMaxAmount else nearMissMaxAmount\n",
    "    clfGlobals.nearMiss_samples = NearMissAmountToTake\n",
    "    print(f\"take {NearMissAmountToTake} near misses\")\n",
    "    # take near misses for this classifier\n",
    "    data_csv_negatives_NearMiss = data_csv_negatives_nearMiss.sample(n=NearMissAmountToTake)\n",
    "\n",
    "    # take random negatives that aren't near miss\n",
    "    negatives_amount_left_to_take = lower_amount - NearMissAmountToTake\n",
    "    #lev- bug fix: assert should be valid if left expression also \"equals 0\"\n",
    "    assert (negatives_amount_left_to_take >= 0)\n",
    "    rest_of_negatives = data_csv.loc[\n",
    "        ~data_csv['label'].isin([label, clfGlobals.nearMissLabel])]  # take all valid rows\n",
    "\n",
    "    negatives_lower_amount_samples = data_csv_negatives_NearMiss.append(\n",
    "        rest_of_negatives.sample(n=negatives_amount_left_to_take))\n",
    "    assert (len(negatives_lower_amount_samples) == lower_amount)\n",
    "    # prepare for results tracking\n",
    "\n",
    "    \"\"\"\n",
    "    # clfGlobals.positives = lower_amount\n",
    "    # clfGlobals.negatives = lower_amount\n",
    "\n",
    "    #  positives - taking random rows\n",
    "    data_csv_positives = data_csv[data_csv.label == label]\n",
    "    # create pandas dataframe with lower_amount rows randomly\n",
    "    positives_lower_amount_samples = data_csv_positives.sample(n=lower_amount)\n",
    "    \n",
    "    # take negatives\n",
    "    negatives = data_csv.loc[\n",
    "    ~data_csv['label'].isin([label])]  # take all valid rows  ~data_csv['label'].isin([label, clfGlobals.nearMissLabel])]   \n",
    "    negatives_lower_amount= negatives.sample(n=lower_amount)\n",
    "    \n",
    "    # combine\n",
    "    combined_lower_amount = positives_lower_amount_samples\n",
    "    # have to assign, returns appended datadrame\n",
    "    combined_lower_amount = combined_lower_amount.append(negatives_lower_amount)\n",
    "    # print(len(combined_lower_amount))  # 734 ,when lower bound: 367\n",
    "\n",
    "    \"\"\"\"\n",
    "    dont need  safe override and data analysis in evaluation process\n",
    "    # saving pandas dataframe to csv - for data analysis purposes\n",
    "    #  TODO lev future - maybe build a function- you already copied this logic 3 times\n",
    "    if os.path.exists(screamGlobals.path_csv_train_test_data):\n",
    "        text = input(f'Press the space bar to override {screamGlobals.path_csv_train_test_data} and continue with the script')\n",
    "        if text != ' ':\n",
    "            sys.exit('User aborted script, pickle file saved :)')\n",
    "    combined_lower_amount.to_csv(screamGlobals.path_csv_train_test_data)\n",
    "\n",
    "    #TODO RETURN HERE LINES OF CODE FOR EDITING LABELS \n",
    "\n",
    "    # saving pandas dataframe to pickle - modularity\n",
    "    #  prevent pickle file over run by accident\n",
    "    if os.path.exists(screamGlobals.csv_to_pkl_path):\n",
    "        text = input(f'Press the space bar to override {screamGlobals.csv_to_pkl_path} and continue with the script')\n",
    "        if text != ' ':\n",
    "            sys.exit('User aborted script, pickle file saved :)')\n",
    "    combined_lower_amount.to_pickle(screamGlobals.csv_to_pkl_path)\n",
    "    \"\"\"\n",
    "\n",
    "    assert (len(combined_lower_amount) == lower_amount * 2)\n",
    "    combined_lower_amount.to_csv(clf_train_test_data)\n",
    "    return combined_lower_amount\n",
    "\n",
    "def get_scaled(np_df):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np_df)  # must call fit before calling transform.fitting on train, using on train+test+valid\n",
    "    scaled = scaler.transform(np_df)    \n",
    "    return scaled\n",
    "\n",
    "\n",
    "def plot_math_medians():\n",
    "    samples =  pd.read_csv(clfGlobals.path_csv_train_test_data)\n",
    "    pos = samples[samples.label == clfGlobals.get_clf_label()]\n",
    "    neg = samples[samples.label != clfGlobals.get_clf_label()]\n",
    "    pos = pos.loc[:, ~pos.columns.isin(['Unnamed: 0','filename','label'])]  # take only features\n",
    "    neg = neg.loc[:, ~neg.columns.isin(['Unnamed: 0','filename','label'])]  # take only features\n",
    "    np_pos= pos.to_numpy()\n",
    "    np_neg= neg.to_numpy()\n",
    "    np_pos_scaled_mean= np.mean(get_scaled(np_pos),axis=0,dtype=np.float64)\n",
    "    np_neg_scaled_mean= np.mean(get_scaled(np_neg),axis=0,dtype=np.float64)\n",
    "\n",
    "    features = pos.columns\n",
    "    # plot\n",
    "    min_a=np.min(np_pos_scaled_mean)\n",
    "    max_a=np.max(np_pos_scaled_mean)\n",
    "    min_b=np.min(np_neg_scaled_mean)\n",
    "    max_b=np.max(np_neg_scaled_mean)\n",
    "    min_t=np.minimum(min_b,min_a )\n",
    "    max_t=np.maximum(max_b,max_a )\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(50, 5))\n",
    "    ax = plt.subplot(132)\n",
    "    ax.set_xticklabels(features, rotation=30)\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([min_t, max_t])\n",
    "\n",
    "    plt.scatter(features, np_pos_scaled_mean,c='green')\n",
    "    plt.scatter(features, np_neg_scaled_mean,c='red')\n",
    "    plt.suptitle('Mean of Features: Positive vs Negative')\n",
    "    ax.set_xlabel('Features', fontsize=12)\n",
    "    ax.set_ylabel('Mean value', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('features_mean-Pos_vs_Neg')\n",
    "    plt.show()\n",
    "    \n",
    "def normalize(x, axis=0):\n",
    "    return minmax_scale(x, axis=axis)    \n",
    "\n",
    "\n",
    "def create_image_dataset_from_wav_dataset(source=\"train\"):\n",
    "    \"\"\"\n",
    "    input: No input.\n",
    "    output: Void, but extracts features by \"plot\", and saves into destination accordint to the next\n",
    "            structure: \n",
    "            <folder of wav files>/<folder of wav files>_images/ <name of wav file>_<feature type>.jpeg\n",
    "            for rxample: train/positive/scream/1.wav   ==> save an image with the files features as plot in\n",
    "                         train/positive/scream/scream_images/1_MFCC.jpeg\n",
    "    \"\"\"\n",
    "    # reaching each wav file\n",
    "    path_train = Path(source)\n",
    "    for path_label in sorted(path_train.iterdir()):\n",
    "        print(\"currently in : \" + str(path_label))  # train\\negative\n",
    "        positiveOrNegative = path_label.name  # negative\n",
    "        #  print(label)\n",
    "        for path_class in tqdm(sorted(path_label.iterdir())):\n",
    "            # print info\n",
    "            print(\"currently in class: \" + str(path_class))\n",
    "            # print amount of files in directory\n",
    "            onlyfiles = next(os.walk(path_class))[2]  # dir is your directory path as string\n",
    "            wav_amount: int = len(onlyfiles)\n",
    "            print(\"wav amount= \" + str(wav_amount))\n",
    "            #  true_class= path_class.name\n",
    "            #  print(true_class)\n",
    "            #  print(path_class)  #  train\\negative\\scream\n",
    "            #  print(\"name: \"+ str(path_class.name))\n",
    "\n",
    "            # lev improvement according to coordination with mori- irrelevant since 7.8.19\n",
    "            if (positiveOrNegative == \"positive\"):\n",
    "                label = path_class.name  # scream\n",
    "            else:\n",
    "                \"\"\"\n",
    "                lev- updating to differentiate near misses and far misses.\n",
    "                keeping if-else structure for future options\n",
    "\n",
    "                old:\n",
    "                print(f\"switching label from {path_class.name} to <negative>\")  # added reporting\n",
    "                label = \"negative\"\n",
    "                new:\n",
    "\n",
    "                \"\"\"\n",
    "                label = path_class.name  # NearMiss_scream\n",
    "\n",
    "            wave_file_paths = path_class.glob('**/*.wav')  # <class 'generator'>\n",
    "            #  print(type(wave_file_paths))\n",
    "            count = 0  # for progress tracking\n",
    "            print('covered WAV files to images: ')\n",
    "            for wav_path in sorted(wave_file_paths):\n",
    "                wav_path = Path(wav_path)\n",
    "                count += 1\n",
    "                if (count % 50) == 0:\n",
    "                    fp = sys.stdout\n",
    "                    print(str(count), end=' ')\n",
    "                    fp.flush()  # makes print flush its buffer (doesnt print without it)\n",
    "                #  print(type(wav_path))  #  <class 'pathlib.WindowsPath'>\n",
    "                #  print(wav_path)  #  train\\positive\\scream\\110142__ryding__scary-scream-4.wav\n",
    "                #  print(wav_path.name)  #  110142__ryding__scary-scream-4.wav\n",
    "                try:\n",
    "                    wav_data, sampling_rate = librosa.load(wav_path, duration=5, sr=22050)\n",
    "                    img_path = str(path_class) + \"\\\\\" + str(path_class.name) +\"_images\\\\\" +str(wav_path.stem)\n",
    "                    #print(img_path)\n",
    "                    extract_features(wav_data,sampling_rate,img_path )\n",
    "\n",
    "                except NoBackendError as e:\n",
    "                    print(\"audioread.NoBackendError \" + \"for wav path \" + str(wav_path))\n",
    "                    continue  # one file didnt work, continue to next one\n",
    "                    \n",
    "\n",
    "def extract_features(wav_data,sampling_rate,img_path):\n",
    "    \"\"\"\n",
    "    wrapper for extracting features as images\n",
    "    \"\"\"\n",
    "    extract_save_mfccs(wav_data,sampling_rate,img_path)\n",
    "\n",
    "def extract_save_mfccs(wav_data,sampling_rate,img_path):\n",
    "    img_path_save = img_path + \"_MFCC.jpeg\"\n",
    "    # print('img_path_save= ' + img_path_save)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax_6 = plt.subplot(2, 1, 1)\n",
    "    mfccs = librosa.feature.mfcc(y=wav_data, sr=sampling_rate)\n",
    "    librosa.display.specshow(normalize(mfccs), x_axis='time', sr=sampling_rate)\n",
    "\n",
    "    plt.suptitle('Normalized MFCCS',y=1.03,x=0.43)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(img_path_save, bbox_inches='tight')\n",
    "    # plt.show()    \n",
    "    \n",
    "    \n",
    "def create_entire_dataset():\n",
    "    \"\"\"\n",
    "    creates the entire dataset.images and csv file.\n",
    "    \"\"\"\n",
    "    clfGlobals = global_For_Clf('scream')\n",
    "    create_csv(clfGlobals.data_file_path, clfGlobals.min_wav_duration, clfGlobals.n_mfcc, clfGlobals.csv_initial_head, clfGlobals.n_mfcc)\n",
    "    create_lower_bound_data_panda(clfGlobals.data_file_path, clfGlobals.get_clf_label(), clfGlobals.try_lower_amount, clfGlobals.path_csv_train_test_data)  \n",
    "    create_image_dataset_from_wav_dataset()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create csv with features \n",
    "clfGlobals = global_For_Clf('scream')\n",
    "create_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_lower_bound_data_panda(clfGlobals.data_file_path, clfGlobals.get_clf_label())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "show feature values differences between 'scream' & 'far miss' (Math aspect in Math vs Image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_math_medians() includes this and the next cell\n",
    "\n",
    "samples =  pd.read_csv(clfGlobals.path_csv_train_test_data)\n",
    "pos = samples[samples.label == clfGlobals.get_clf_label()]\n",
    "neg = samples[samples.label != clfGlobals.get_clf_label()]\n",
    "pos = pos.loc[:, ~pos.columns.isin(['Unnamed: 0','filename','label'])]  # take only features\n",
    "neg = neg.loc[:, ~neg.columns.isin(['Unnamed: 0','filename','label'])]  # take only features\n",
    "np_pos= pos.to_numpy()\n",
    "np_neg= neg.to_numpy()\n",
    "np_pos_scaled_mean= np.mean(get_scaled(np_pos),axis=0,dtype=np.float64)\n",
    "np_neg_scaled_mean= np.mean(get_scaled(np_neg),axis=0,dtype=np.float64)\n",
    "\n",
    "features = pos.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "min_a=np.min(np_pos_scaled_mean)\n",
    "max_a=np.max(np_pos_scaled_mean)\n",
    "min_b=np.min(np_neg_scaled_mean)\n",
    "max_b=np.max(np_neg_scaled_mean)\n",
    "min_t=np.minimum(min_b,min_a )\n",
    "max_t=np.maximum(max_b,max_a )\n",
    "\n",
    "\n",
    "plt.figure(figsize=(50, 5))\n",
    "ax = plt.subplot(132)\n",
    "ax.set_xticklabels(features, rotation=30)\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([min_t, max_t])\n",
    "\n",
    "plt.scatter(features, np_pos_scaled_mean,c='green')\n",
    "plt.scatter(features, np_neg_scaled_mean,c='red')\n",
    "plt.suptitle('Mean of Features: Positive vs Negative')\n",
    "ax.set_xlabel('Features', fontsize=12)\n",
    "ax.set_ylabel('Mean value', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('features_mean-Pos_vs_Neg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show differences in Images from Positives & Negatives (Image aspect in Image vs Math)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quick check of audio- how it sounds\n",
    "pos_wav_path = \"train\\positive\\scream\\\\1_scream_female_room.wav\"\n",
    "#ipd.Audio(filename=pos_wav_path,rate=22050)\n",
    "neg_wav_path = \"train\\\\negative\\\\Far_miss\\\\0d20191008163744pnull_sec_start_0.wav\"\n",
    "ipd.Audio(filename=neg_wav_path,rate=22050)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_paths = [pos_wav_path, neg_wav_path ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral centroids in red\n",
    "pos_wav_data, pos_sampling_rate = librosa.load(pos_wav_path, duration=5, sr=22050)\n",
    "neg_wav_data, neg_sampling_rate = librosa.load(neg_wav_path, duration=5, sr=22050)\n",
    "# Normalising the spectral centroid for visualisation\n",
    "def normalize(x, axis=0):\n",
    "    return minmax_scale(x, axis=axis)\n",
    "\n",
    "feature_spectralCentroid = librosa.feature.spectral_centroid(y=neg_wav_data, sr=pos_sampling_rate)[0]\n",
    "# feature_wav_spec_cent.shape  # (1, 120)\n",
    "# compute time for visualization\n",
    "frames = range(len(feature_spectralCentroid))\n",
    "t = librosa.frames_to_time(frames)  # shape (120,)\n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "ax_2 = plt.subplot(133)\n",
    "# plt.suptitle('Normalized Spectral Centroids- Positive vs Negative')\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveplot(neg_wav_data, sr=neg_sampling_rate,\n",
    "                         alpha=0.4,label='Negative(Chat)',color= 'r')\n",
    "# plt.suptitle('neg')\n",
    "plt.plot(t, normalize(feature_spectralCentroid), color='r'\n",
    "        ,label='Spec_cent (Negative)')\n",
    "#plt.savefig('plots/neg_test' +'_spectral_centroids.jpeg', bbox_inches='tight')\n",
    "\n",
    "### ### ###\n",
    "feature_spectralCentroid = librosa.feature.spectral_centroid(y=pos_wav_data, sr=pos_sampling_rate)[0]\n",
    "# feature_wav_spec_cent.shape  # (1, 120)\n",
    "# compute time for visualization\n",
    "frames = range(len(feature_spectralCentroid))\n",
    "t = librosa.frames_to_time(frames)  # shape (120,)\n",
    "\n",
    "\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveplot(pos_wav_data, sr=neg_sampling_rate, alpha=0.6,\n",
    "                         label='Positive(Scream)',color= 'g')\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(t, normalize(feature_spectralCentroid), color='g'\n",
    "        ,label='Spec_cent (Positive)')\n",
    "\n",
    "# ax.set_xlabel('Features', fontsize=12)\n",
    "ax_2.set_title('Normalized Spectral Centroids- Positive vs Negative')\n",
    "ax_2.set_ylabel('Normalized value', fontsize=12)\n",
    "ax_2.set_xlabel('Time(sec)', fontsize=12)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig('images/spectral_centroids-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero crossing rate experiments..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero crossings\n",
    "#Plot the signal:\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveplot(neg_wav_data, sr=neg_sampling_rate)\n",
    "# Zooming in\n",
    "n0 = 9000\n",
    "n1 = 9100\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(neg_wav_data[n0:n1])\n",
    "plt.grid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = 17000\n",
    "n1 = 18500\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(pos_wav_data[n0:n1])\n",
    "plt.suptitle('pos')\n",
    "plt.grid()\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.suptitle('neg')\n",
    "plt.plot(neg_wav_data[n0:n1])\n",
    "plt.grid()\n",
    "\n",
    "feature_spectralCentroid = librosa.feature.spectral_centroid(y=neg_wav_data[n0:n1], sr=pos_sampling_rate)[0]\n",
    "# feature_wav_spec_cent.shape  # (1, 120)\n",
    "# compute time for visualization\n",
    "frames = range(len(feature_spectralCentroid))\n",
    "t = librosa.frames_to_time(frames)  # shape (120,)\n",
    "\n",
    "# Normalising the spectral centroid for visualisation\n",
    "def normalize(x, axis=0):\n",
    "    return minmax_scale(x, axis=axis)\n",
    "\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveplot(neg_wav_data[n0:n1], sr=neg_sampling_rate, alpha=0.4)\n",
    "plt.plot(t, feature_spectralCentroid, color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_crossings = librosa.zero_crossings(pos_wav_data[n0:n1], pad=False)\n",
    "print('pos= ',sum(zero_crossings))#16\n",
    "\n",
    "zero_crossings = librosa.zero_crossings(neg_wav_data[n0:n1], pad=False)\n",
    "print('neg= ',sum(zero_crossings))#16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(pos_wav_data[n0:17500],rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(neg_wav_data[n0:17500],rate=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine pos and neg to 1 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_ZCR = librosa.feature.zero_crossing_rate(y=pos_wav_data)\n",
    "feature_ZCR.shape  # (1, 120)\n",
    "# compute time for visualization\n",
    "frames = range(len(feature_ZCR))\n",
    "t = librosa.frames_to_time(frames)  # shape (120,)\n",
    "\n",
    "# Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveplot(pos_wav_data, sr=neg_sampling_rate, alpha=0.6,\n",
    "                         label='Positive(Scream)',color= 'g')\n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "ax_3 = plt.subplot(133)\n",
    "\n",
    "plt.plot(t, normalize(feature_ZCR), color='g'\n",
    "         ,label='Spec_cent (Positive)')\n",
    "\n",
    "ax_3.set_title('Zero crossing rate- Positive vs Negative')\n",
    "ax_3.set_ylabel('Normalized value', fontsize=12)\n",
    "ax_3.set_xlabel('Time (sec)', fontsize=12)\n",
    "\n",
    "#plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.savefig('images/spectral_centroids-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax_3 = plt.subplot(111)\n",
    "\n",
    "feature_ZCR = librosa.feature.zero_crossing_rate(y=neg_wav_data)\n",
    "frames = range(len(feature_ZCR[0]))\n",
    "t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "plt.plot(t, normalize(feature_ZCR[0]), color='r'\n",
    "        ,label='ZCR (Negative)')\n",
    "librosa.display.waveplot(neg_wav_data, sr=neg_sampling_rate,\n",
    "                         alpha=0.4,label='Negative(Chat)',color= 'r')\n",
    "## now for positive\n",
    "feature_ZCR = librosa.feature.zero_crossing_rate(y=pos_wav_data)\n",
    "frames = range(len(feature_ZCR[0]))\n",
    "t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "plt.plot(t, normalize(feature_ZCR[0]), color='g'\n",
    "        ,label='ZCR (Positive)')\n",
    "librosa.display.waveplot(pos_wav_data, sr=pos_sampling_rate,\n",
    "                         alpha=0.6,label='Positive(Scream)',color= 'g')\n",
    "\n",
    "\n",
    "# finishing visualization\n",
    "ax_3.set_title('Normalized Zero Crossing Rate- Positive vs Negative')\n",
    "ax_3.set_ylabel('Normalized rate', fontsize=12)\n",
    "ax_3.set_xlabel('Time (sec)', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/ZCR-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spectral rolloff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax_4 = plt.subplot(122)\n",
    "\n",
    "feature_SRO = librosa.feature.spectral_rolloff(y=neg_wav_data)\n",
    "frames = range(len(feature_SRO[0]))\n",
    "t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "plt.plot(t, normalize(feature_SRO[0]), color='r'\n",
    "        ,label='SRO (Negative)')\n",
    "librosa.display.waveplot(neg_wav_data, sr=neg_sampling_rate,\n",
    "                         alpha=0.4,label='Negative(Chat)',color= 'r')\n",
    "## now for positive\n",
    "feature_SRO = librosa.feature.spectral_rolloff(y=pos_wav_data)\n",
    "frames = range(len(feature_SRO[0]))\n",
    "t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "plt.plot(t, normalize(feature_SRO[0]), color='g'\n",
    "        ,label='SRO (Positive)')\n",
    "librosa.display.waveplot(pos_wav_data, sr=pos_sampling_rate,\n",
    "                         alpha=0.6,label='Positive(Scream)',color= 'g')\n",
    "\n",
    "\n",
    "# finishing visualization\n",
    "ax_4.set_title('Normalized Spectral rolloff- Positive vs Negative')\n",
    "ax_4.set_ylabel('Normalized value', fontsize=12)\n",
    "ax_4.set_xlabel('Time (sec)', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/SRO-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chromagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(211)\n",
    "\n",
    "feature_chromagram = librosa.feature.chroma_stft(y=neg_wav_data, sr=neg_sampling_rate)\n",
    "librosa.display.specshow(feature_chromagram, x_axis='time', y_axis='chroma')\n",
    "plt.colorbar()\n",
    "plt.ylabel('Scales')\n",
    "# finishing visualization\n",
    "plt.subplot(122)\n",
    "feature_chromagram_true = librosa.feature.chroma_stft(y=pos_wav_data, sr=pos_sampling_rate)\n",
    "librosa.display.specshow(feature_chromagram_true, x_axis='time', y_axis='chroma')\n",
    "plt.colorbar()\n",
    "plt.ylabel('Scales')\n",
    "plt.tight_layout()\n",
    "\n",
    "# ax_5[0].set_title('Cromagram scales intensity- Negative')\n",
    "# ax_5[0].set_ylabel(, fontsize=12)\n",
    "# ax_5[0].set_xlabel('Time (sec)', fontsize=12)\n",
    "\n",
    "# # finishing visualization\n",
    "\n",
    "# ax_5[1].set_title('Cromagram scales intensity- Positive')\n",
    "# ax_5[1].set_ylabel('Scales', fontsize=12)\n",
    "# ax_5[1].set_xlabel('Time (sec)', fontsize=12)\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.tight_layout()\n",
    "# # plt.savefig('images/Cromagram-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax_6 = plt.subplot(2, 1, 1)\n",
    "\n",
    "ax_6.set_title('Negative', color='r')\n",
    "librosa.display.specshow(feature_chromagram, x_axis='time', y_axis='chroma')\n",
    "plt.colorbar()\n",
    "# plt.suptitle('a')\n",
    "plt.ylabel('Scale')\n",
    "ax_7 = plt.subplot(2, 1, 2,)\n",
    "librosa.display.specshow(feature_chromagram_true, x_axis='time', y_axis='chroma')\n",
    "plt.colorbar()\n",
    "plt.ylabel('Scale')\n",
    "ax_7.set_title('Positive', color='g')\n",
    "plt.suptitle('Chromagram scales - Positive vs Negative',y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Cromagram-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  rms (and mfccs) \n",
    "n_mfcc = 20  # resolution amount\n",
    "mfccs = librosa.feature.mfcc(y=neg_wav_data, sr=neg_sampling_rate, n_mfcc=n_mfcc)\n",
    "S, phase = librosa.magphase(mfccs)\n",
    "rms = librosa.feature.rms(S=S)\n",
    "\n",
    "mfccs = librosa.feature.mfcc(y=pos_wav_data, sr=pos_sampling_rate, n_mfcc=n_mfcc)\n",
    "S, phase = librosa.magphase(mfccs)\n",
    "rms_pos = librosa.feature.rms(S=S)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax_6 = plt.subplot(2, 1, 1)\n",
    "plt.semilogy(rms.T, label='RMS Negative Energy',color='r')\n",
    "plt.xticks([])\n",
    "plt.xlim([0, rms.shape[-1]])\n",
    "\n",
    "plt.semilogy(rms_pos.T, label='RMS Positive Energy', color='g')\n",
    "plt.xticks([])\n",
    "plt.xlim([0, rms_pos.shape[-1]])\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "# ax_7 = plt.subplot(2, 1, 2,)\n",
    "# librosa.display.specshow(feature_chromagram_true, x_axis='time', y_axis='chroma')\n",
    "# plt.colorbar()\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Time')\n",
    "# ax_7.set_title('Positive', color='g')\n",
    "plt.suptitle('RMS energy - Positive vs Negative',y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/RMS_energy-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax_6 = plt.subplot(2, 1, 1)\n",
    "ax_6.set_title('Negative', color='r')\n",
    "S = librosa.feature.melspectrogram(y=neg_wav_data, sr=neg_sampling_rate)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB, x_axis='time',\n",
    "                          y_axis='mel', sr=neg_sampling_rate,\n",
    "                          fmax=8000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "\n",
    "ax_7 = plt.subplot(2, 1, 2,)\n",
    "S = librosa.feature.melspectrogram(y=pos_wav_data, sr=pos_sampling_rate)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB, x_axis='time',\n",
    "                          y_axis='mel', sr=pos_sampling_rate,\n",
    "                          fmax=8000)\n",
    "plt.colorbar(format='%+2.0f dB')\n",
    "ax_7.set_title('Positive', color='g')\n",
    "plt.suptitle('Mel-frequency spectrogram- Positive vs Negative',y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Mel_spec-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "ax_6 = plt.subplot(2, 1, 1)\n",
    "ax_6.set_title('Negative', color='r')\n",
    "mfccs = librosa.feature.mfcc(y=neg_wav_data, sr=neg_sampling_rate)\n",
    "librosa.display.specshow(normalize(mfccs), x_axis='time', sr=neg_sampling_rate)\n",
    "plt.colorbar()\n",
    "\n",
    "ax_7 = plt.subplot(2, 1, 2,)\n",
    "mfccs_pos = librosa.feature.melspectrogram(y=pos_wav_data, sr=pos_sampling_rate)\n",
    "librosa.display.specshow(normalize(mfccs_pos), x_axis='time', sr=pos_sampling_rate)\n",
    "plt.colorbar()\n",
    "ax_7.set_title('Positive', color='g')\n",
    "plt.suptitle('Normalized MFCCS- Positive vs Negative',y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/Normalized_MFCCS-Pos_vs_Neg.jpeg', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my code snippet experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "wav_name = \"train\\positive\\scream\\\\1_scream_female_room.wav\"\n",
    "print(wav_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(filename=wav_name,rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "wav_data, sampling_rate = librosa.load(wav_name, duration=5, sr=22050)\n",
    "chroma_stft = librosa.feature.chroma_stft(y=wav_data, sr=sampling_rate)\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(chroma_stft, y_axis='chroma', x_axis='time')\n",
    "plt.colorbar()\n",
    "plt.title('Chromagram')\n",
    "plt.tight_layout()\n",
    "plt.savefig('chroma')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_stft.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an image dataset from wav files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use mfcc's first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_image_dataset_from_wav_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_entire_dataset() # using same images 1:1 ratio POS : NEG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's create the models and try to train_test and see precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image models...based on (edited, not copied!)\n",
    "https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experimenting with image datagenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2)\n",
    "#         rotation_range=40,\n",
    "#         width_shift_range=0.2,\n",
    "#         # height_shift_range=0.2,  # not logical for audio\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         # horizontal_flip=True, # not logical for audio- reverse scream?\n",
    "#         fill_mode='nearest')\n",
    "\n",
    "        \n",
    "        \n",
    "wav_p = \"train\\positive\\scream\\scream_images\\\\1_scream_female_room_MFCC.jpeg\"\n",
    "img = load_img(wav_p)  # this is a PIL image\n",
    "x = img_to_array(img)  # this is a Numpy array with shape (3, 150, 150)\n",
    "x = x.reshape((1,) + x.shape)  # this is a Numpy array with shape (1, 3, 150, 150)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "# and saves the results to the `preview/` directory\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1,\n",
    "                          save_to_dir='preview', save_prefix='', save_format='jpeg'):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break  # otherwise the generator would loop indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_1_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 (preparations)\n",
    "preparations for experiment 3\n",
    "\n",
    "goal: compare different feature images and see who gives the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "feature= \"zcr\"\n",
    "feature_path= \"data\\\\test\\\\test_\" + feature\n",
    "Path(feature_path).mkdir(parents=False, exist_ok=True) # dont create new parents, dont raise error if destination exists\n",
    "scream_path= feature_path+\"\\\\scream\"\n",
    "Path(scream_path).mkdir(parents=False, exist_ok=True) # dont create new parents, dont raise error if destination exists\n",
    "Far_Miss_path= feature_path+\"\\\\Far_Miss\"\n",
    "Path(Far_Miss_path).mkdir(parents=False, exist_ok=True) # dont create new parents, dont raise error if destination exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# goal: compare different feature images and see who gives the best results\n",
    "#exp_3\n",
    "clfGlobals = global_For_Clf('scream')\n",
    "feature_names = [\"SpecCent\",\"Zcr\",\"SpecRol\",\"Chro\",\"Rms\",\"MelSpec\"]\n",
    "feature_accuracies= []\n",
    "# create dataset for all the above features\n",
    "create_image_train_test_dataset_by_features(feature_names,clfGlobals.n_mfcc,source=\"train\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataset as well\n",
    "clfGlobals = global_For_Clf('scream')\n",
    "feature_names = [\"SpecCent\",\"Zcr\",\"SpecRol\",\"Chro\",\"Rms\",\"MelSpec\"]\n",
    "create_image_train_test_dataset_by_features(feature_names,clfGlobals.n_mfcc,source=\"test_wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compare results: same \"image receiving\" model, but every time it will learn from a single feature\n",
    "\n",
    "feature_names = [\"SpecCent\",\"Zcr\",\"SpecRol\",\"Chro\",\"Rms\",\"MelSpec\"]\n",
    "epochs_vector= [50]\n",
    "for train_epochs in epochs_vector:\n",
    "    accuracies= []\n",
    "    for feature in feature_names:\n",
    "        accuracies.append(train_save_getRes_val_test(feature,train_epochs=train_epochs))\n",
    "\n",
    "    acc_validation= [x[0] for x in accuracies]\n",
    "    acc_test= [x[1] for x in accuracies]\n",
    "\n",
    "    plot_exp_3(feature_names,acc_validation, acc_test, train_epochs)\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chose not to run this because my goal was to see if images can win in accuracy when given the\n",
    "# same epoch amount.\n",
    "clfGlobals = global_For_Clf('scream')\n",
    "feature_names = [\"SpecCent\",\"Zcr\",\"SpecRol\",\"Chro\",\"Rms\",\"MelSpec\"]\n",
    "feature_accuracies= []\n",
    "# create dataset for all the above features\n",
    "create_image_train_test_dataset_by_features(\"MFCC\",clfGlobals.n_mfcc,source=\"train\")    \n",
    "create_image_train_test_dataset_by_features(\"MFCC\",clfGlobals.n_mfcc,source=\"test_wav\")\n",
    "feature_names = [\"SpecCent\",\"Zcr\",\"SpecRol\",\"Chro\",\"Rms\",\"MelSpec\",\"MFCC\"]\n",
    "epochs_vector= [100]\n",
    "for train_epochs in epochs_vector:\n",
    "    accuracies= []\n",
    "    for feature in feature_names:\n",
    "        accuracies.append(train_save_getRes_val_test(feature,train_epochs=train_epochs))\n",
    "\n",
    "    acc_validation= [x[0] for x in accuracies]\n",
    "    acc_test= [x[1] for x in accuracies]\n",
    "\n",
    "    plot_exp_3(feature_names,acc_validation, acc_test, train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking to see that logic is working.success.\n",
    "\n",
    "# compare results: same \"image receiving\" model, but every time it will learn from a single feature\n",
    "feature_names = [\"SpecCent\",\"Zcr\",\"SpecRol\",\"Chro\",\"Rms\",\"MelSpec\"]\n",
    "train_epochs= 1\n",
    "accuracies= []\n",
    "for feature in feature_names:\n",
    "    accuracies.append(train_save_getRes_val_test(feature,train_epochs=train_epochs))\n",
    "\n",
    "acc_validation= [x[0] for x in accuracies]\n",
    "acc_test= [x[1] for x in accuracies]\n",
    "\n",
    "plot_exp_3(feature_names,acc_validation, acc_test, train_epochs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exp_3(feature_names,acc_validation, acc_test, train_epochs):\n",
    "    plt.figure(figsize=(30, 5))\n",
    "    ax = plt.subplot(132)\n",
    "    plt.plot(feature_names,acc_validation,label='Validation', color='yellow')\n",
    "    plt.plot(feature_names,acc_test,label='Test', color='purple')\n",
    "\n",
    "    header= 'Image models accuracies for '+ str(train_epochs) + ' training epochs'\n",
    "    plt.suptitle(header)\n",
    "    plt.legend(loc='lower right')\n",
    "    ax.set_xlabel('Feature', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    p_save='images/exp_3_Image_feature_accuracies_Epochs_'+str(train_epochs)\n",
    "    plt.savefig(p_save)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prove of concept.\n",
    "feature_names = [\"SpecCent\",\"Zcr\"]\n",
    "train_epochs= 1\n",
    "ti=[[10,0],[11,1]]\n",
    "acc_validation= [x[0] for x in ti]\n",
    "acc_test= [x[1] for x in ti]\n",
    "print(acc_validation,acc_test)\n",
    "\n",
    "plt.figure(figsize=(30, 5))\n",
    "ax = plt.subplot(132)\n",
    "plt.plot(feature_names,acc_validation,label='validation', color='yellow')\n",
    "plt.plot(feature_names,acc_test,label='test', color='purple')\n",
    "\n",
    "header= 'Image models accuracies for '+ str(train_epochs) + ' training epochs'\n",
    "plt.suptitle(header)\n",
    "plt.legend(loc='lower right')\n",
    "ax.set_xlabel('Feature', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontsize=12)\n",
    "\n",
    "\n",
    "# plt.tight_layout()\n",
    "plt.savefig('images/exp_3_Image_feature_accuracies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_getRes_val_test(feature,train_epochs=50):\n",
    "    acc_val_test= [[],[]]\n",
    "    val_acc_slot= 0\n",
    "    test_acc_slot=1\n",
    "\n",
    "    batch_size = 5\n",
    "\n",
    "    # this is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=False)  # without this- reverse audio isnt relevant\n",
    "\n",
    "    # this is the augmentation configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolers of 'data/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    path_train= 'data/train/train_' + str(feature)\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        path_train,  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "\n",
    "    # this is a similar generator, for validation data\n",
    "    path_val= 'data/validation/validation_' + str(feature)\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        path_val,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "    input_shape_image= (150,150,3)\n",
    "    model= experiment_1_get_model(input_shape_image)\n",
    "    history= model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=train_epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "    path_model= 'weights/exp_3/weights_'+str(feature)+'.h5'\n",
    "    model.save_weights(path_model)  # always save your weights after training or during training\n",
    "\n",
    "    # print(\"val_acc: \", history.history['accuracy'])\n",
    "    acc_val_test[val_acc_slot]= history.history['accuracy'][-1]\n",
    "\n",
    "\n",
    "    # test set\n",
    "    total_samples= 50 #test- TODO- dont hard code..\n",
    "    # batch_size = 5\n",
    "    STEP_SIZE_VALID= total_samples // batch_size\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    # this is a similar generator, for validation data\n",
    "    path_test= 'data/test/test_' + str(feature)\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        path_test,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "    history_test= model.evaluate_generator(generator=test_generator, steps=STEP_SIZE_VALID)\n",
    "    acc_val_test[test_acc_slot]= history_test[test_acc_slot]\n",
    "    print(\"feature: \", feature,\". acc: [validation, test]: \", acc_val_test)\n",
    "    return acc_val_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"feature: \", feature,\". acc: [validation, test]: \", acc_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "\n",
    "def create_image_train_test_dataset_by_features(feature_names,mfcc_amount,source=\"train\"):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    # reaching each wav file\n",
    "    path_train = Path(source)\n",
    "    for path_label in sorted(path_train.iterdir()):\n",
    "        print(\"currently in : \" + str(path_label))  # train\\negative\n",
    "        positiveOrNegative = path_label.name  # negative\n",
    "        #  print(label)\n",
    "        for path_class in tqdm(sorted(path_label.iterdir())):\n",
    "            # print info\n",
    "            print(\"currently in class: \" + str(path_class))\n",
    "            # print amount of files in directory\n",
    "            onlyfiles = next(os.walk(path_class))[2]  # dir is your directory path as string\n",
    "            wav_amount: int = len(onlyfiles)\n",
    "            print(\"wav amount= \" + str(wav_amount))\n",
    "            #  true_class= path_class.name\n",
    "            #  print(true_class)\n",
    "            #  print(path_class)  #  train\\negative\\scream\n",
    "            #  print(\"name: \"+ str(path_class.name))\n",
    "\n",
    "            # lev improvement according to coordination with mori- irrelevant since 7.8.19\n",
    "            if (positiveOrNegative == \"positive\"):\n",
    "                label = path_class.name  # scream\n",
    "            else:\n",
    "                \"\"\"\n",
    "                lev- updating to differentiate near misses and far misses.\n",
    "                keeping if-else structure for future options\n",
    "\n",
    "                old:\n",
    "                print(f\"switching label from {path_class.name} to <negative>\")  # added reporting\n",
    "                label = \"negative\"\n",
    "                new:\n",
    "\n",
    "                \"\"\"\n",
    "                label = path_class.name  # NearMiss_scream\n",
    "\n",
    "            wave_file_paths = path_class.glob('**/*.wav')  # <class 'generator'>\n",
    "            #  print(type(wave_file_paths))\n",
    "            count = 0  # for progress tracking\n",
    "            print('covered WAV files to images: ')\n",
    "            \n",
    "            #TODO can improve copying logic of files. for now its hard coded\n",
    "            train_amount= 300 # its 90% of 376~\n",
    "            current_copied= 0 # when this reaches train_amount we copy files to validation\n",
    "            for wav_path in sorted(wave_file_paths):\n",
    "                wav_path = Path(wav_path)\n",
    "                count += 1\n",
    "                if (count % 50) == 0:\n",
    "                    fp = sys.stdout\n",
    "                    print(str(count), end=' ')\n",
    "                    fp.flush()  # makes print flush its buffer (doesnt print without it)\n",
    "                #  print(type(wav_path))  #  <class 'pathlib.WindowsPath'>\n",
    "                #  print(wav_path)  #  train\\positive\\scream\\110142__ryding__scary-scream-4.wav\n",
    "                #  print(wav_path.name)  #  110142__ryding__scary-scream-4.wav\n",
    "                try:\n",
    "                    wav_data, sampling_rate = librosa.load(wav_path, duration=5, sr=22050)\n",
    "                    \n",
    "                    for feature in feature_names:\n",
    "                        dest_dir_path=str(path_class) + \"\\\\\" + str(path_class.name) +\"_images_\"+ str(feature) \n",
    "                        Path(dest_dir_path).mkdir(parents=False, exist_ok=True) # dont create new parents, dont raise error if destination exists\n",
    "                        img_path = str(dest_dir_path) + str(wav_path.stem)\n",
    "                        #print(img_path)\n",
    "                        img_saved_path= extract_save_by_feature(wav_data,sampling_rate,img_path,feature,mfcc_amount)   \n",
    "                        \n",
    "                        #new function?\n",
    "                        if source == \"train\":\n",
    "                            if current_copied <= train_amount:\n",
    "                                dest_img_path= \"data\\\\train\\\\train_\"+feature+\"\\\\\"+label\n",
    "                                Path(dest_img_path).mkdir(parents=True, exist_ok=True) #create missing parent directories\n",
    "                                img_dst_path= dest_img_path+\"\\\\\"+str(wav_path.stem)+\".jpeg\"\n",
    "                                copyfile(img_saved_path, img_dst_path)\n",
    "\n",
    "                            else:\n",
    "                                dest_img_path= \"data\\\\validation\\\\validation_\"+feature+\"\\\\\"+label\n",
    "                                Path(dest_img_path).mkdir(parents=True, exist_ok=True) #create missing parent directories\n",
    "                                img_dst_path= dest_img_path+\"\\\\\"+str(wav_path.stem)+\".jpeg\"\n",
    "                                copyfile(img_saved_path, img_dst_path)\n",
    "                        else:\n",
    "                            dest_img_path= \"data\\\\test\\\\test_\"+feature+\"\\\\\"+label\n",
    "                            Path(dest_img_path).mkdir(parents=True, exist_ok=True) #create missing parent directories\n",
    "                            img_dst_path= dest_img_path+\"\\\\\"+str(wav_path.stem)+\".jpeg\"\n",
    "                            copyfile(img_saved_path, img_dst_path)\n",
    "                            \n",
    "                    current_copied+=1\n",
    "                    \n",
    "                except NoBackendError as e:\n",
    "                    print(\"audioread.NoBackendError \" + \"for wav path \" + str(wav_path))\n",
    "                    continue  # one file didnt work, continue to next one\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_save_by_feature(wav_data,sampling_rate,img_path,feature,mfcc_amount):\n",
    "    img_path_save = img_path +\"_\"+str(feature)+\".jpeg\"\n",
    "    # print('img_path_save= ' + img_path_save)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    ax_6 = plt.subplot(2, 1, 1)\n",
    "    \n",
    "    if feature == \"MFCC\":\n",
    "        mfccs = librosa.feature.mfcc(y=wav_data, sr=sampling_rate)\n",
    "        librosa.display.specshow(normalize(mfccs), x_axis='time', sr=sampling_rate)\n",
    "    \n",
    "    elif feature == \"SpecCent\":\n",
    "        feature_spectralCentroid = librosa.feature.spectral_centroid(y=wav_data, sr=sampling_rate)[0]\n",
    "        frames = range(len(feature_spectralCentroid))\n",
    "        t = librosa.frames_to_time(frames)  # shape (120,)\n",
    "        plt.plot(t,normalize(feature_spectralCentroid))\n",
    "        \n",
    "    elif feature == \"Zcr\":\n",
    "            feature_ZCR = librosa.feature.zero_crossing_rate(y=wav_data)\n",
    "            frames = range(len(feature_ZCR[0]))\n",
    "            t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "            plt.plot(t, normalize(feature_ZCR[0]))\n",
    "            \n",
    "    elif feature == \"SpecRol\": \n",
    "            feature_SRO = librosa.feature.spectral_rolloff(y=wav_data)\n",
    "            frames = range(len(feature_SRO[0]))\n",
    "            t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "            plt.plot(t, normalize(feature_SRO[0]))\n",
    "            \n",
    "    elif feature == \"Chro\":\n",
    "            feature_chromagram = librosa.feature.chroma_stft(y=wav_data, sr=sampling_rate)\n",
    "            librosa.display.specshow(feature_chromagram, x_axis='time', y_axis='chroma')     \n",
    "            \n",
    "    elif feature == \"Rms\":\n",
    "            mfccs = librosa.feature.mfcc(y=wav_data, sr=sampling_rate, n_mfcc=mfcc_amount)\n",
    "            S, phase = librosa.magphase(mfccs)\n",
    "            rms = librosa.feature.rms(S=S)\n",
    "            plt.semilogy(rms.T)\n",
    "            plt.xticks([])\n",
    "            plt.xlim([0, rms.shape[-1]])\n",
    "            \n",
    "    elif feature == \"MelSpec\":\n",
    "            S = librosa.feature.melspectrogram(y=wav_data, sr=sampling_rate)\n",
    "            S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "            librosa.display.specshow(S_dB, x_axis='time',\n",
    "                          y_axis='mel', sr=sampling_rate,\n",
    "                          fmax=8000)\n",
    "            \n",
    "\n",
    "    plt.suptitle(feature,y=1.03,x=0.43)        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(img_path_save, bbox_inches='tight')\n",
    "    # plt.show()   \n",
    "    return img_path_save\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifying appearance for aboce cell (function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(y=wav_data, sr=sampling_rate)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB, x_axis='time',\n",
    "                          y_axis='mel', sr=sampling_rate,\n",
    "                          fmax=8000)\n",
    "\n",
    "plt.suptitle(\"Mel\",y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"testing_7.jpeg\", bbox_inches='tight')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_amount = 20  # resolution amount\n",
    "mfccs = librosa.feature.mfcc(y=wav_data, sr=sampling_rate, n_mfcc=mfcc_amount)\n",
    "S, phase = librosa.magphase(mfccs)\n",
    "rms = librosa.feature.rms(S=S)\n",
    "plt.semilogy(rms.T)\n",
    "plt.xticks([])\n",
    "plt.xlim([0, rms.shape[-1]])\n",
    "\n",
    "plt.suptitle(\"RMS\",y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"testing_5.jpeg\", bbox_inches='tight')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_chromagram = librosa.feature.chroma_stft(y=wav_data, sr=sampling_rate)\n",
    "librosa.display.specshow(feature_chromagram, x_axis='time', y_axis='chroma')\n",
    "\n",
    "plt.suptitle(\"CHRO\",y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"testing_4.jpeg\", bbox_inches='tight')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_SRO = librosa.feature.spectral_rolloff(y=wav_data)\n",
    "frames = range(len(feature_SRO[0]))\n",
    "t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "plt.plot(t, normalize(feature_SRO[0]))\n",
    "plt.suptitle(\"SPECT\",y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"testing_3.jpeg\", bbox_inches='tight')\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_ZCR = librosa.feature.zero_crossing_rate(y=wav_data)\n",
    "frames = range(len(feature_ZCR[0]))\n",
    "t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "plt.plot(t, normalize(feature_ZCR[0]))\n",
    "\n",
    "#plt.suptitle(\"zcr\",y=1.03,x=0.43)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"testing_2.jpeg\", bbox_inches='tight')\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spectralCentroid = librosa.feature.spectral_centroid(y=wav_data, sr=sampling_rate)[0]\n",
    "frames = range(len(feature_spectralCentroid))\n",
    "t = librosa.frames_to_time(frames)  # shape (120,)\n",
    "plt.plot(t,normalize(feature_spectralCentroid))\n",
    "plt.suptitle(\"SpecCent\",y=1.03,x=0.43)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"testing.jpeg\", bbox_inches='tight')\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spectralCentroid = librosa.feature.spectral_centroid(y=neg_wav_data, sr=pos_sampling_rate)[0]\n",
    "# feature_wav_spec_cent.shape  # (1, 120)\n",
    "# compute time for visualization\n",
    "frames = range(len(feature_spectralCentroid))\n",
    "t = librosa.frames_to_time(frames)  # shape (120,)\n",
    "\n",
    "plt.figure(figsize=(25,5))\n",
    "ax_2 = plt.subplot(133)\n",
    "# plt.suptitle('Normalized Spectral Centroids- Positive vs Negative')\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveplot(neg_wav_data, sr=neg_sampling_rate,\n",
    "                         alpha=0.4,label='Negative(Chat)',color= 'r')\n",
    "# plt.suptitle('neg')\n",
    "plt.plot(t, normalize(feature_spectralCentroid), color='r'\n",
    "        ,label='Spec_cent (Negative)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in the above cells to show a visual process...\n",
    "# exp_3()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preparations- draw one image with 3 features in 2 graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "ax_6 = plt.subplot(2, 1, 1)\n",
    "ax_6.set_title('MelSpec')\n",
    "S = librosa.feature.melspectrogram(y=wav_data, sr=sampling_rate)\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "librosa.display.specshow(S_dB, x_axis='time',\n",
    "              y_axis='mel', sr=sampling_rate,\n",
    "              fmax=8000)\n",
    "ax_7 = plt.subplot(2, 1, 2)\n",
    "ax_7.set_title('SpecRol(Red), Rms(Green)')\n",
    "mfccs = librosa.feature.mfcc(y=wav_data, sr=sampling_rate, n_mfcc=mfcc_amount)\n",
    "S, phase = librosa.magphase(mfccs)\n",
    "rms = librosa.feature.rms(S=S)\n",
    "plt.plot(t, normalize(rms[0]), color='g'\n",
    "        ,label='Rms')\n",
    "feature_SRO = librosa.feature.spectral_rolloff(y=wav_data)\n",
    "frames = range(len(feature_SRO[0]))\n",
    "t = librosa.frames_to_time(frames)  # shape (130,)\n",
    "plt.plot(t, normalize(feature_SRO[0]), color='r'\n",
    "        ,label='SpecRol')\n",
    "ax_7.set_ylabel('Normalized value', fontsize=12)\n",
    "ax_7.set_xlabel('Time (sec)', fontsize=12)\n",
    "\n",
    "plt.suptitle('Exp_4- best 3 features',y=0.95,x=0.50)\n",
    "plt.savefig('images/exp_4_test.jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    clfGlobals = global_For_Clf('scream')\n",
    "    feature_names = [\"MelSpec_Rms_SpecRol\"] \n",
    "    # create dataset for all the above features\n",
    "    create_image_train_test_dataset_by_features(feature_names,clfGlobals.n_mfcc,source=\"train\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "   # because i didn't use scatter while plotting a single feature doesnt draw anything,\n",
    "    # that's why i printed afterwards the results...\n",
    "    # for a small experiment that's ok but for future enhancement\n",
    "    # of course i'll need to fix this..\n",
    "    \n",
    "    clfGlobals = global_For_Clf('scream')\n",
    "    feature_names = [\"MelSpec_Rms_SpecRol\"] \n",
    "    # create test dataset as well\n",
    "    create_image_train_test_dataset_by_features(feature_names,clfGlobals.n_mfcc,source=\"test_wav\")\n",
    "    # compare results: same \"image receiving\" model, but every time it will learn from a single feature\n",
    "    epochs_vector= [50]\n",
    "    for train_epochs in epochs_vector:\n",
    "        accuracies= []\n",
    "        for feature in feature_names:\n",
    "            accuracies.append(train_save_getRes_val_test(feature,train_epochs=train_epochs))\n",
    "\n",
    "        acc_validation= [x[0] for x in accuracies]\n",
    "        acc_test= [x[1] for x in accuracies]\n",
    "\n",
    "        plot_exp_3(feature_names,acc_validation, acc_test, train_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beginning of unsuccessful attempt- keeping this section for documentation of my mistakes.\n",
    "\n",
    "Basically I didn't understand the correct flow of this spesific approach in \n",
    "Transfer learning, so I couldn't figure out how to properly utilize it.\n",
    "\n",
    "What I understand now (and this is the correct flow) is that:\n",
    "1. you load a pretrained model with its predefined weights.\n",
    "2. you pass ALL future samples (train,validation AND TEST) to the pretrained model.\n",
    "3. the output from the pretrained model are FEATURES that are actually the:\n",
    "4. input for the \"top model\" which outputs the class prediction.\n",
    "\n",
    "* my mistake was at stage 3. i couldn't understand how to \"predict\" correctly...:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensions of our images.\n",
    "img_width, img_height = 150, 150\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'data/train/train_MelSpec'\n",
    "validation_data_dir = 'data/validation/validation_MelSpec'\n",
    "nb_train_samples = 602   # 301 per class\n",
    "nb_validation_samples = 132  # 66 per class\n",
    "epochs = 50\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "# build the VGG16 network\n",
    "model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "bottleneck_features_train = model.predict_generator(\n",
    "    generator, nb_train_samples // batch_size)\n",
    "# np.save(open('bottleneck_features_train.npy', 'w'),bottleneck_features_train)\n",
    "\n",
    "generator = datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode=None,\n",
    "    shuffle=False)\n",
    "bottleneck_features_validation = model.predict_generator(\n",
    "    generator, nb_validation_samples // batch_size)\n",
    "# np.save(open('bottleneck_features_validation.npy', 'w'), bottleneck_features_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = bottleneck_features_train\n",
    "train_labels = np.array(\n",
    "    [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "validation_data = bottleneck_features_validation\n",
    "validation_labels = np.array(\n",
    "    [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels.shape, validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=train_data.shape[1:]))  # take from second place (0 is first)\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels,\n",
    "          epochs=epochs,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(validation_data, validation_labels))\n",
    "model.save_weights(top_model_weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of unsuccessful attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for some reson this want succesful...val accuracy of 0 and 1 and i cant predict on my own data so it doesnt worth much...trying a different tutorial to learn from:\n",
    "https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load my mel spec model\n",
    "input_shape_image= (150,150,3)\n",
    "model_melSpec= experiment_1_get_model(input_shape_image)\n",
    "model_melSpec.load_weights('weights\\exp_3\\weights_MelSpec.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.applications import vgg16\n",
    "from keras.models import Model\n",
    "#import keras\n",
    "\n",
    "vgg = applications.VGG16(include_top=False, weights='imagenet', \n",
    "                                     input_shape=input_shape_image)\n",
    "\n",
    "output = vgg.layers[-1].output\n",
    "output = Flatten()(output)\n",
    "vgg_model = Model(vgg.input, output)\n",
    "\n",
    "vgg_model.trainable = False\n",
    "for layer in vgg_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "import pandas as pd\n",
    "pd.set_option('max_colwidth', -1)\n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "\n",
    "IMG_DIM = (150, 150)\n",
    "\n",
    "train_files = glob.glob('data/train/train_MelSpec/Far_miss/*',recursive=True)\n",
    "train_files.extend(glob.glob('data/train/train_MelSpec/scream/*',recursive=True))\n",
    "print(len(train_files))\n",
    "train_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in train_files]\n",
    "train_imgs = np.array(train_imgs)\n",
    "\n",
    "print(train_labels)\n",
    "validation_files = glob.glob('data/validation/validation_MelSpec/Far_miss/*',recursive=True)\n",
    "validation_files.extend(glob.glob('data/validation/validation_MelSpec/scream/*',recursive=True))\n",
    "validation_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in validation_files]\n",
    "validation_imgs = np.array(validation_imgs)\n",
    "\n",
    "\n",
    "print('Train dataset shape:', train_imgs.shape, \n",
    "      '\\tValidation dataset shape:', validation_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs_scaled = train_imgs.astype('float32')\n",
    "validation_imgs_scaled  = validation_imgs.astype('float32')\n",
    "train_imgs_scaled /= 255\n",
    "validation_imgs_scaled /= 255\n",
    "\n",
    "print(train_imgs[0].shape)\n",
    "array_to_img(train_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples = 602   # 301 per class\n",
    "nb_validation_samples = 132  # 66 per class\n",
    "\n",
    "train_labels = np.array(\n",
    "    [0] * (nb_train_samples // 2) + [1] * (nb_train_samples // 2))\n",
    "\n",
    "validation_labels = np.array(\n",
    "    [0] * (nb_validation_samples // 2) + [1] * (nb_validation_samples // 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_feature_example = vgg.predict(train_imgs_scaled[0:1])\n",
    "print(bottleneck_feature_example.shape)\n",
    "plt.imshow(bottleneck_feature_example[0][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bottleneck_features(model, input_imgs):\n",
    "    features = model.predict(input_imgs, verbose=0)\n",
    "    return features\n",
    "    \n",
    "train_features_vgg = get_bottleneck_features(vgg_model, train_imgs_scaled)\n",
    "validation_features_vgg = get_bottleneck_features(vgg_model, validation_imgs_scaled)\n",
    "\n",
    "print('Train Bottleneck Features:', train_features_vgg.shape, \n",
    "      '\\tValidation Bottleneck Features:', validation_features_vgg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.output_shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "input_shape = vgg_model.output_shape[1] # model will receive the output of the pretrained model without\n",
    "                                        # th top layer (without the prediction)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(input_shape,)))\n",
    "model.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 2\n",
    "epochs= 50\n",
    "\n",
    "train_labels_enc = train_labels\n",
    "validation_labels_enc = validation_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_features_vgg, y=train_labels_enc,\n",
    "                    validation_data=(validation_features_vgg, validation_labels_enc),\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Far_Miss_scream_tlearn_basic_cnn.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IMG_DIM = (150, 150)\n",
    "test_files = glob.glob('data/test/test_MelSpec/Far_miss/*',recursive=True)\n",
    "test_files.extend(glob.glob('data/test/test_MelSpec/scream/*',recursive=True))\n",
    "print(len(test_files))\n",
    "test_imgs = [img_to_array(load_img(img, target_size=IMG_DIM)) for img in test_files]\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_imgs_scaled = test_imgs.astype('float32')\n",
    "test_imgs_scaled /= 255\n",
    "print(test_imgs_scaled[0].shape)\n",
    "array_to_img(test_imgs_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_amount= 50\n",
    "test_labels = np.array(\n",
    "    [0] * (test_amount // 2) + [1] * (test_amount // 2))\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_features_vgg = get_bottleneck_features(vgg_model, test_imgs_scaled)\n",
    "    test_features_vgg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    predictions = model.predict_classes(test_features_vgg)\n",
    "    # evaluate accuracy\n",
    "    acc = accuracy_score(test_labels, predictions)\n",
    "    print('Test Accuracy for basic VGG16 transfer learning:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained CNN model as a Feature Extractor with Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3,  \n",
    "                                   horizontal_flip=False, fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(train_imgs, train_labels_enc, batch_size=2)\n",
    "val_generator = val_datagen.flow(validation_imgs, validation_labels_enc, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wont extract the bottleneck features like last time since we will be training on data generators; hence, we will be passing the vgg_model object as an input to our own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "nb_train_samples = 602   # 301 per class\n",
    "nb_validation_samples = 132  # 66 per class\n",
    "batch_size= 2\n",
    "\n",
    "model_2 = Sequential()\n",
    "model_2.add(vgg_model) # the VGG-16 models layers are still frozen\n",
    "# we are still using it as a basic feature extractor only.\n",
    "# vgg_model.output_shape[1] = 8192\n",
    "model_2.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model_2.add(Dropout(0.3))\n",
    "model_2.add(Dense(512, activation='relu'))\n",
    "model_2.add(Dropout(0.3))\n",
    "model_2.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "              metrics=['accuracy'])\n",
    "# We bring the learning rate slightly down since \n",
    "# we will be training for 100 epochs and dont want to\n",
    "# make any sudden abrupt weight adjustments to our model layers.\n",
    "              \n",
    "history = model_2.fit_generator(train_generator, steps_per_epoch= nb_train_samples//batch_size ,\n",
    "                              epochs=100,\n",
    "                              validation_data=val_generator, \n",
    "                              validation_steps=nb_validation_samples//batch_size , \n",
    "                              verbose=1)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.save('Far_Miss_scream_tlearn_img_aug_cnn.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_2 = model_2.predict_classes(test_imgs_scaled, verbose=0)\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(test_labels, predictions_2)\n",
    "print('Test Accuracy for VGG16 transfer learning with image Augmentation:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if the model was the same model i used previousely, but now it will have the pretrained model ...\n",
    "the answer is that you can't put conv2d after dense on same branch so it didn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model_2_2 = Sequential()\n",
    "# model_2_2.add(vgg_model)\n",
    "# model_2_2.add(Dense(512, activation='relu', input_dim=input_shape)) \n",
    "\n",
    "# model_2_2.add(Conv2D(32, (3, 3)))\n",
    "# model_2_2.add(Activation('relu'))\n",
    "# model_2_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model_2_2.add(Conv2D(32, (3, 3)))\n",
    "# model_2_2.add(Activation('relu'))\n",
    "# model_2_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model_2_2.add(Conv2D(64, (3, 3)))\n",
    "# model_2_2.add(Activation('relu'))\n",
    "# model_2_2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# model_2_2.add(Flatten())\n",
    "# model_2_2.add(Dense(64))\n",
    "# model_2_2.add(Activation('relu'))\n",
    "# model_2_2.add(Dropout(0.5))\n",
    "# model_2_2.add(Dense(1))\n",
    "# model_2_2.add(Activation('sigmoid'))\n",
    "\n",
    "# model_2_2.compile(loss='binary_crossentropy',\n",
    "#           optimizer=optimizers.RMSprop(lr=2e-5),\n",
    "#           metrics=['accuracy'])\n",
    "\n",
    "# history = model_2_2.fit_generator(train_generator, steps_per_epoch= nb_train_samples//batch_size ,\n",
    "#                               epochs=50,\n",
    "#                               validation_data=val_generator, \n",
    "#                               validation_steps=nb_validation_samples//batch_size , \n",
    "#                               verbose=1)   \n",
    "\n",
    "# model_2_2.save('Far_Miss_scream_tlearn_img_aug_cnn_MyModel.h5') \n",
    "\n",
    "# predictions_2_2 = model_2_2.predict_classes(test_imgs_scaled, verbose=0)\n",
    "# # evaluate accuracy\n",
    "# acc = accuracy_score(test_labels, predictions_2_2)\n",
    "# print('Test Accuracy for VGG16 transfer learning with image Augmentation My Model:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-trained CNN model with Fine-tuning and Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model.trainable = True\n",
    "\n",
    "set_trainable = False\n",
    "for layer in vgg_model.layers:\n",
    "    if layer.name in ['block5_conv1', 'block4_conv1']:\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "        \n",
    "layers = [(layer, layer.name, layer.trainable) for layer in vgg_model.layers]\n",
    "pd.DataFrame(layers, columns=['Layer Type', 'Layer Name', 'Layer Trainable'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3,\n",
    "                                   horizontal_flip=False, fill_mode='nearest')\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow(train_imgs, train_labels_enc, batch_size=2)\n",
    "val_generator = val_datagen.flow(validation_imgs, validation_labels_enc, batch_size=2)\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers\n",
    "\n",
    "model_3 = Sequential()\n",
    "model_3.add(vgg_model)\n",
    "model_3.add(Dense(512, activation='relu', input_dim=input_shape))\n",
    "model_3.add(Dropout(0.3))\n",
    "model_3.add(Dense(512, activation='relu'))\n",
    "model_3.add(Dropout(0.3))\n",
    "model_3.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_3.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-5),\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model_3.fit_generator(train_generator, steps_per_epoch= nb_train_samples//batch_size,\n",
    "                              epochs=50,     \n",
    "                              validation_data=val_generator, \n",
    "                              validation_steps=nb_validation_samples//batch_size , \n",
    "                              verbose=1) \n",
    "model_3.save('cats_dogs_tlearn_finetune_img_aug_cnn.h5')\n",
    "predictions_3 = model_3.predict_classes(test_imgs_scaled, verbose=0)\n",
    "# evaluate accuracy\n",
    "acc = accuracy_score(test_labels, predictions_3)\n",
    "print('Test Accuracy for VGG16 transfer learning with image Augmentation and fine tuning:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
